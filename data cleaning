import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download the stopwords from NLTK
nltk.download('punkt')
nltk.download('stopwords')

def clean_text(text):
    # Remove URLs
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
    # Remove mentions and hashtag
    text = re.sub(r'\@\w+|\#','', text)
    # Tokenize the words
    tokenized = word_tokenize(text)
    # Remove the stop words
    tokenized = [token for token in tokenized if token not in stopwords.words('english')]
    # Remove punctuation and non-alphabetic characters
    cleaned_text = [word for word in tokenized if word.isalpha()]
    return ' '.join(cleaned_text)

# Load your CSV file
df = pd.read_csv('/content/train.csv')

# Check the column names in your DataFrame
print(df.columns)

# Replace 'actual_text_column' with the actual name of the column containing the text data
df['cleaned_text'] = df['text'].apply(clean_text)

# Save the cleaned data to a new CSV file
df.to_csv('cleaned_data.csv', index=False)

